library(tidyverse)
#vectors
typeof(c(1L , 3L))
c(2.5, 48.5, 101.5)
c(TRUE, FALSE, TRUE)
x <- c(33.5, 57.75, 120.05)
length(x)
y <- c(TRUE, TRUE, FALSE)
is.character(y)
typeof(y)
#Explicit coercion as.logical(), as.integer(), as.double(), or as.character()
#list
a <- (list("a", 1L, 1.5, TRUE))
str(a)
z <- list(list(list(1 , 3, 5)))
# $ symbols reflect the nested structure of this list.
xa <- sample(20, 100, replace = TRUE)
ya <- x > 10
sum(ya)  # how many are greater than 10?
mean(ya) # what proportion are greater than 10?
xa <- sample(20, 100, replace = TRUE)
ya <- x > 10
sum(ya)  # how many are greater than 10?
mean(ya) # what proportion are greater than 10?
sample(10) + 100
xa <- sample(20, 100, replace = TRUE)
xa
ya <- xa > 10
sum(ya)  # how many are greater than 10?
mean(ya) # what proportion are greater than 10?
sample(10) + 100
library(lubridate)
today()
now()
ymd("2021-01-20")
as_date(now())
data.frame(x = c(1, 2, 3) , y = c(1.5, 5.5, 7.5))
matrix(c(3:8), ncol = 2)
matrix(c(3:8), nrow = 2)
as_date(now())
now()
#ggplot
head(diamond)
head(diamonds)
ggplot(data = diamonds, aes(x = carat, y = price, color = cut)) +
geom_point() +
facet_wrap(~cut)
#facet_wrap() is an R function used to create subplots
glimpse(diamonds)
browseVignettes(tidyverse)
browseVignettes("tidyverse")
#pipe
data("toothgrowth")
#pipe
data("ToothGrowth")
filltered_tg <- filter(ab,dose==0.5)
ab <- data("ToothGrowth")
filltered_tg <- filter(ab,dose==0.5)
filltered_tg <- filter(ab,dose==0.5)
filltered_tg <- filter(ToothGrowth,dose==0.5)
data("ToothGrowth")
filltered_tg <- filter(ToothGrowth,dose==0.5)
head(filltered_tg)
Filltered_tooth <- ToothGrowth %>%
filter(dose==0.5)
Filltered_tooth <- ToothGrowth %>%
filter(dose==0.5) %>%
arrange(len)
head(Filltered_tooth)
Filltered_tooth <- ToothGrowth %>%
filter(dose==0.5) %>%
group_by(supp)
arrange(len)
Filltered_tooth <- ToothGrowth %>%
filter(dose==0.5) %>%
group_by(supp) %>%
arrange(len)
Filltered_tooth <- ToothGrowth %>%
filter(dose==0.5) %>%
group_by(supp) %>%
arrange(len)
head(Filltered_tooth)
Filltered_tooth_sum <- ToothGrowth %>%
filter(dose==0.5) %>%
group_by(supp) %>%
summarize(mean_len= mean_len = na.rm = T),group="drop"
Filltered_tooth_sum <- ToothGrowth %>%
filter(dose==0.5) %>%
group_by(supp) %>%
summarize(mean_len= mean_len, na.rm = T),group="drop"
Filltered_tooth_sum <- ToothGrowth %>%
filter(dose==0.5) %>%
group_by(supp) %>%
summarize(mean_len= mean(len, na.rm = T)),group="drop")
Filltered_tooth_sum <- ToothGrowth %>%
filter(dose==0.5) %>%
group_by(supp) %>%
summarize(mean_len= mean(len, na.rm = T)),group="drop")
Filltered_tooth_sum <- ToothGrowth %>%
filter(dose==0.5) %>%
group_by(supp) %>%
summarize(mean_len = mean(len,na.rm = T)),group="drop")
Filltered_tooth_sum <- ToothGrowth %>%
filter(dose==0.5) %>%
group_by(supp) %>%
summarize(mean_len = mean(len,na.rm = T),group="drop")
head(Filltered_tooth_sum)
#dataframe
mutate(diamonds,carat_2=carat*100)
str(diamonds)
glimpse(diamonds)
as_tibble(diamonds)
kabble(diamonds)
kable(diamonds)
# read_excel
read.csv(https://d3c33hcgiwev3.cloudfront.net/GL0bk8O2Sja9G5PDtko2uQ_31e445d7ca64417eb45aeaa08ec90bf1_hotel_bookings.csv?Expires=1626652800&Signature=lXJrpJ5Mx44STUuGy~04GVW2cDKenB5uds5n9hFBTce2V7w3mmadLVRqxlsVp1rOYS-0zZE1PtwOVTb8ruJr56ruSAtvLh9rd8r0nkW9rsGrV9wWVu25Ju1fV7INxfWeo7g7QvzsD2IGY3BQKZfz50KvlD9XE~TFs93U~c0VPG8_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A,head = T)
# read_excel
read.csv("https://d3c33hcgiwev3.cloudfront.net/GL0bk8O2Sja9G5PDtko2uQ_31e445d7ca64417eb45aeaa08ec90bf1_hotel_bookings.csv?Expires=1626652800&Signature=lXJrpJ5Mx44STUuGy~04GVW2cDKenB5uds5n9hFBTce2V7w3mmadLVRqxlsVp1rOYS-0zZE1PtwOVTb8ruJr56ruSAtvLh9rd8r0nkW9rsGrV9wWVu25Ju1fV7INxfWeo7g7QvzsD2IGY3BQKZfz50KvlD9XE~TFs93U~c0VPG8_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A",head = T)
# read_excel
read.csv("hotel_bookings.csv")
# read_excel
hotel_booking <- read.csv("~/Users/danielkent/Downloads/hotel_bookings.csv")
# read_excel
hotel_booking <- read.csv("~//Downloads/hotel_bookings.csv")
hotel_booking
glimpse(hotel_booking)
# read_excel
bookings_df <- hotel_booking <- read.csv("~//Downloads/hotel_bookings.csv")
glimpse(hotel_booking)
new_df <- select(bookings_df, `adr`, adults)
mutate(new_df, total = `adr` / adults)
# Cleaning data
install.packages("skimr")
install.packages("janitor")
p_unload(skimr,janitor,dplr)
p_load(skimr,janitor,dplr)
p_load(skimr,janitor,tidyverse)
p_load(skimr,janitor,tidyverse,palmerpenguins)
skim_without_charts(penguins)
glimpse(penguins)
penguins %>%
select(-species)
#everything except species
clean_names(penguins)
#everything except species
rename_with(penguins,tolow)
#everything except species
rename_with(penguins,tolower)
#new data
bookings_df <- hotel_booking <- read.csv("~//Downloads/hotel_bookings.csv")
glimpse(booking)df
glimpse(booking_df)
bookings_df <- hotel_booking <- read.csv("~//Downloads/hotel_bookings.csv")
glimpse(booking_df)
glimpse(bookings_df)
trimmed_df %>%
select(hotel, is_canceled, lead_time) %>%
rename( = hotel)
trimmed_df %>%
select(hotel, is_canceled, lead_time) %>%
rename( hotel_type= hotel)
trimmed_df <- bookings_df %>%
select( , , )
trimmed_df <- bookings_df %>%
select(hotel , is_canceled, lead_time)
trimmed_df %>%
select(hotel, is_canceled, lead_time) %>%
rename( hotel_type= hotel)
#date unite
example_df <- bookings_df %>%
select(arrival_date_year, arrival_date_month) %>%
unite(arrival_month_year, c("arrival_date_month", "arrival_date_year"), sep = " ")
glimpse(bookings_df)
example_df <- bookings_df %>%
mutate(guests =adults,children,babies )
head(example_df)
head(guests)
head(guests)
head(example_df(guests)
head(example_df(guests))
,
example_df <- bookings_df %>%
summarize(number_canceled= sum(is_canceled),average_lead_time= avg(lead_time) )
example_df <- bookings_df %>%
summarize(number_canceled= sum(is_canceled),average_lead_time= avg(lead_time) )
example_df <- bookings_df %>%
summarize(number_canceled= sum(is_canceled),average_lead_time= mean(lead_time) )
head(example_df)
penguins %>%
arrange(bill_length_mm)
install.packages('Tmisc')
library(Tmisc)
data(quartet)
glimpse(quartet)
quartet %>%
group_by(set) %>%
summarize(mean(x),sd(x),cor(x,y))
install.packages('datasauRus')
library(datasauRus)
ggplot(quartet,aes(x,y)) + geom_point() + geom_smooth()
ggplot(quartet,aes(x,y)) + geom_point() + geom_smooth(method = lm,se = F)
ggplot(quartet,aes(x,y)) + geom_point() + geom_smooth(method = lm,se = F)+
facet_wrap(~set)
ggplot(datasaurus_dozen,aes(x=x,y=y,colour=dataset))+ geom_point() +theme_void()
+theme()
ggplot(datasaurus_dozen,aes(x=x,y=y,colour=dataset))+ geom_point() +theme_void()
+theme()+ facet_wrap(~dataset)
ggplot(datasaurus_dozen,aes(x=x,y=y,colour=dataset))+ geom_point() +theme_void()
+theme()+ facet_wrap(~dataset)
datasaurus_dozen %>%
ggplot(aes(x, y, color = dataset)) +
geom_point(show.legend = FALSE) +
facet_wrap(~dataset, ncol = 5)
hotel_summary <-
hotel_bookings %>%
group_by(hotel) %>%
summarise(average_lead_time=mean(lead_time),
min_lead_time=min(lead_time),
max_lead_time=max(lead_time))
hotel_summary <-
bookings_df %>%
group_by(hotel) %>%
summarise(average_lead_time=mean(lead_time),
min_lead_time=min(lead_time),
max_lead_time=max(lead_time))
head(hotel_summary)
Library(tidyverse)
library(skimr)
library(tidyverse)
library(skimr)
library(janitor)
hotel_bookings <- read_csv("https://d3c33hcgiwev3.cloudfront.net/GL0bk8O2Sja9G5PDtko2uQ_31e445d7ca64417eb45aeaa08ec90bf1_hotel_bookings.csv?Expires=1626652800&Signature=lXJrpJ5Mx44STUuGy~04GVW2cDKenB5uds5n9hFBTce2V7w3mmadLVRqxlsVp1rOYS-0zZE1PtwOVTb8ruJr56ruSAtvLh9rd8r0nkW9rsGrV9wWVu25Ju1fV7INxfWeo7g7QvzsD2IGY3BQKZfz50KvlD9XE~TFs93U~c0VPG8_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A")
head(hotel_bookings)
glimpse(hotel_booking)
glimpse(hotel_bookings)
hotel_city<- filter(hotel_bookings, hotel_bookings$hotel=="City Hotel")
head(hotel_city)
glimpse(hotel_city)
hotel_summary <-
hotel_bookings %>%
group_by(hotel) %>%
summarise(average_lead_time=mean(lead_time),
min_lead_time=min(lead_time),
max_lead_time=max(lead_time))
head(hotel_summary)
mean(hotel_bookings_v2$lead_time)
hotel_bookings_v2 <-
arrange(hotel_bookings, desc(lead_time))
mean(hotel_bookings_v2$lead_time)
ggplot(data = hotel_bookings) +
geom_bar(mapping = aes(x = distribution_channel)) +
facet_wrap(~deposit_type~market_segment) +
theme(axis.text.x = element_text(angle = 45))
ggplot(data = hotel_bookings) +
geom_bar(mapping = aes(x = distribution_channel)) +
facet_grid(~deposit_type) +
theme(axis.text.x = element_text(angle = 45))
ggplot(data = hotel_bookings) +
geom_bar(mapping = aes(x = distribution_channel)) +
facet_wrap(~ )
ggplot(data = hotel_bookings) +
geom_bar(mapping = aes(x = distribution_channel)) +
ggplot(data = hotel_bookings) +
geom_bar(mapping = aes(x = distribution_channel))
ggplot(data = onlineta_city_hotels) +
geom_point(mapping = aes(x = lead_time, y = children))
ggplot(data = hotel_bookings) +
geom_point(mapping = aes(x = lead_time, y = children))
ggplot(data = hotel_bookings,na.rm=T) +
geom_point(mapping = aes(x = lead_time, y = children))
onlineta_city_hotels <- filter(hotel_bookings,
(hotel=="City Hotel" &
hotel_bookings$market_segment=="Online TA"))
onlineta_city_hotels_v2 <- hotel_bookings %>%
filter(hotel=="City Hotel") %>%
filter(market_segment=="Online TA")
#ggplot
ggplot(data = onlineta_city_hotels_v2) +
geom_point(mapping = aes(x = lead_time, y = children))
head(onlineta_city_hotels)
ggplot(data = hotel_bookings) +
geom_bar(mapping = aes(x = market_segment)) +
facet_wrap(~hotel) +
theme(axis.text.x = element_text(angle = 45)) +
labs(title="Comparison of market segments by hotel type for hotel bookings",
caption=paste0("Data from: ", mindate, " to ", maxdate),
x="Market Segment",
y="Number of Bookings")
mindate <- min(hotel_bookings$arrival_date_year)
maxdate <- max(hotel_bookings$arrival_date_year)
ggplot(data = hotel_bookings) +
geom_bar(mapping = aes(x = market_segment)) +
facet_wrap(~hotel) +
theme(axis.text.x = element_text(angle = 45)) +
labs(title="Comparison of market segments by hotel type for hotel bookings",
caption=paste0("Data from: ", mindate, " to ", maxdate),
x="Market Segment",
y="Number of Bookings")
ggsave('hotel_booking_chart.png')
#Title: CISE W17 (Neural Network)
# Author: Kent Daniel
# Contact: B08605042@ntu.edu.tw / kentdaniel.com
# data:https://ceiba.ntu.edu.tw/course/935d63/content/modeldata.txt
#thank you for everything you teach me it mean a lot for me
#package
library(tidyverse)
library(knitr)
library(neuralnet)
library(h2o)
#connect to h2o server
localH2O = h2o.init()
library(NeuralNetTools)
#data
localH2O = h2o.init()
library(NeuralNetTools)
#data
data <- read.table("https://ceiba.ntu.edu.tw/course/935d63/content/modeldata.txt", header=FALSE)
View(data)
colnames(data) <- c("X1", "X2", "Group1", "Group2", "Group3")
#show partial data
kable(head(data), digits = 4)
#plot and grouping data
p_data <- data.frame(data[, 1:2], apply(data[, 3:5], 1, which.max))
# position of the element with the maximal value in a vector
# position of the element with the maximal value in a vector
colnames(p_data) <- c("X1", "X2", "Group")
p_data$Group = factor(p_data$Group)
ggplot(p_data, aes(x = X1, y = X2, col = Group))+
geom_point(alpha = 0.6, size = 2) +
theme_bw() +
labs(x = expression(x[1]), y = expression(x[2])) +
coord_fixed(ratio = 1) +
theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank())
#neural network
nn <- neuralnet(Group1 + Group2 + Group3 ~ X1 + X2, data = data, hidden = 5)
#plot with weights
plot(nn)
pred_nn <- compute(nn, data[, 1:2])
table(p_data$Group, apply(pred_nn$net.result, 1, which.max))
plotnet(nn, pos_col = "green", neg_col = "red", alpha_val = 0.5)
#seperate data into training and test sets
# I learn it from Kaggle titanic dataset
train = p_data[1:500, ]
test = p_data[501:600, ]
#train a neural network model for classification
model = h2o.deeplearning(y = "Group", training_frame = as.h2o(train), validation_frame = as.h2o(test), activation = "Rectifier", hidden = 5, epochs = 6000, train_samples_per_iteration = -2, export_weights_and_biases = T, nfolds = 10, fold_assignment = "Stratified")
#predict groups (neural network model)
pred = h2o.predict(model, as.h2o(p_data[, 1:2]))
#weights of nn model
weight1 = as.data.frame(t(h2o.weights(model, 1)))
rownames(weight1) <- c("X1", "X2")
colnames(weight1) <- c("Hidden 1", "Hidden 2", "Hidden 3", "Hidden 4", "Hidden 5")
kable(weight1, digits = 4, caption = "Weights of 1st layer")
weight2 = as.data.frame(t(h2o.weights(model, 2)))
rownames(weight2) <- c("Hidden 1", "Hidden 2", "Hidden 3", "Hidden 4", "Hidden 5")
colnames(weight2) <- c("Class 1", "Class 2", "Class 3")
kable(weight2, digits = 4, caption = "Weights of 2nd layer")
conf_train <- table(train[, 3], as.data.frame(pred)$predict[1:500])
conf_train
conf_test <- table(test[, 3], as.data.frame(pred)$predict[501:600])
conf_test
#use 10 fold cross validation
model@model$cross_validation_metrics_summary
#plot epoch (classification error)
plot(model)
#grid data
grid <- as.data.frame(expand.grid(seq(min(data[, 1]), max(data[, 1]), length = 1000), seq(min(data[, 2]), max(data[, 2]), length = 1000)))
colnames(grid) <- c("X1", "X2")
pred <- h2o.predict(model, as.h2o(grid))
#plot boundary
ggplot() +
geom_raster(aes(x = grid[, 1],y = grid[, 2], fill = pred), alpha = 0.3, show.legend = F) +
theme_bw() +
geom_point(data = p_data, aes(x = X1, y = X2, color = Group), size = 2) +
labs(title = "Neural network with boundary", x = expression(x[1]), y = expression(x[2])) +
theme(panel.background = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank())
pred <- as.data.frame(pred)$predict
